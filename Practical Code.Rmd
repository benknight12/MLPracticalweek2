---
title: "week 2 ML Practical"
output: html_document
date: "2024-02-01"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
heartdata<- read.csv("heart-adapted.csv")
library(mlr3verse)
head(heartdata)
dim(heartdata)
```

```{r}
heartdata$hd <- factor(heartdata$hd, levels=c("absence","presence"))
head(heartdata)
task <- as_task_classif(heartdata, target='hd', positive = "presence", id = "heart disease")
parts <-partition(task,ratio = 3/4)
dat.train <- heartdata[parts$train,]
dat.holdout <- heartdata[parts$test,]

# define the classification task
tasktrain <- as_task_classif(dat.train, target='hd', positive = "presence", id = "heart disease")
tasktest <- as_task_classif(dat.holdout, target='hd', positive = "presence", id = "heart disease")
# stratify on class variable
task$set_col_roles("hd",c("target","stratum"))
# define the learning algorithm to use. We set scale to TRUE so that each feature is scaled (using the mean/SD of the training set to scale both the training and test set)
knn <- lrn("classif.kknn", predict_type="prob",scale=TRUE)
# 10 fold cross validation
cvscheme <- rsmp("cv", folds=10)
# instantiate the cv scheme â€“ this means we can use the same CV fold split to evaluate both KNN and logistic regression 
cvscheme$instantiate(task)
# Run KNN 10 fold cross validation evaluation
knncv = resample(task, knn, cvscheme, store_models=T)
# calculate average AUC across the 10 folds
meanAUCKnn = knncv$aggregate(msr("classif.auc"))
meanAUCKnn
library(precrec)
autoplot(knncv, type="roc")

## Now try for logreg, repeat steps with different learner
lreg <- lrn("classif.log_reg", predict_type="prob")
lregcv = resample(task, lreg, cvscheme, store_models=T)
meanAUCKnn = lregcv$aggregate(msr("classif.auc"))
meanAUCKnn
autoplot(lregcv, type="roc")

## Check fold split the same
foldsKNN = knncv$resampling$instance
foldsLogistic = lregcv$resampling$instance
all.equal(foldsKNN, foldsLogistic)
## True!
```

```{r}
## Plot roc's together
comp.design = benchmark_grid(
  tasks = task,
  learners = c(knn, lreg),
  resamplings = cvscheme
)
comp = benchmark(comp.design)
autoplot(comp, type = "roc")


## Compare the AUC statistically
aucFoldsKnn = knncv$score(msr("classif.auc"))
aucFoldsLreg = lregcv$score(msr("classif.auc"))

resultsCompare = cbind(aucFoldsKnn[,.(iteration, classif.auc)], aucFoldsLreg[,.(classif.auc)])
colnames(resultsCompare) = c('iteration', 'auc.knn', 'auc.lreg')
resultsCompare
## Express in boxplot form and do a t test
autoplot(knncv, measure = msr("classif.auc"), type = "boxplot")
autoplot(lregcv, measure = msr("classif.auc"), type = "boxplot")
t.test(resultsCompare$auc.knn, y=resultsCompare$auc.lreg, paired=T)
```

```{r}
## Now learn from train set and test on test set for the chosen knn model
task = as_task_classif(heartdata, target="hd", positive="presence", id="heart disease")
knn = lrn("classif.kknn", predict_type="prob")
knn$train(task, parts$train)
responsesHoldout = knn$predict(task, parts$test)
responsesHoldout

library(pROC)
holdoutPreds = responsesHoldout$prob
rocHoldout <- roc(response=responsesHoldout$truth, predictor=holdoutPreds[,"presence"], levels=c("absence","presence"))
rocHoldout

responsesHoldout$score(msrs("classif.auc"))
## Bootstrap to find confidence intervals
boots = ci.auc(rocHoldout, conf.level=0.95, method="bootstrap", boot.n=10000)

## Plot ROC with thresholds
ix = min(which(rocHoldout$thresholds>0.5))
##Index of largest classify with score >0.5
ix
threshSens = rocHoldout$sensitivities[ix]
threshSpec = rocHoldout$specificities[ix]


# plot roc curve and point on it representing classification threshold
g = autoplot(responsesHoldout, type="roc") 
g + geom_point(aes(x=1-threshSpec, y=threshSens), color = "blue")
```
